# -*- coding: utf-8 -*-
"""Assignment 2 data science project sakshi CSEAIML_C, 2021B1531203

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qmM62v-9MMIygUr-TilUrS3HLF3_yGpo
"""

import numpy as np
import pandas as pd

from google.colab import drive
drive.mount('/content/drive')



df_recipes = pd.read_csv('/Titanic-Dataset.csv')

df_recipes.head()

df_recipes.info()

df_recipes.describe()

df_recipes.isnull().sum()

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer

numeric_features=['PassengerId','Survived']
categorical_features = ['Sex	', 'cabin']

numeric_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder())
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num_t', numeric_pipeline, numeric_features),

    ])

X_processed = preprocessor.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)

dataframe_transformed = preprocessor.fit_transform(df_recipes)

dataframe_transformed.shape

df_recipes.info()

preprocessor.named_transformers_

dataframe_transformed = pd.DataFrame(dataframe_transformed)

dataframe_transformed.columns
dataframe_transformed.columns.tolist()

"""enconding

"""

# Encode categorical variables using one-hot encoding
titanic_encoded = pd.get_dummies(df_recipes, columns=['Sex', 'Embarked'], drop_first=True)

# Drop unnecessary columns or features
titanic_encoded.drop(['Name', 'Ticket', 'Cabin'], axis=1, inplace=True)

# Print the first few rows of the encoded dataset
print(titanic_encoded.head())

"""Logistic regression

"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Load the dataset
url = "https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv"
titanic = pd.read_csv(url)

# Drop unnecessary columns
titanic.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)

# Fill missing values
titanic['Age'].fillna(titanic['Age'].median(), inplace=True)
titanic['Embarked'].fillna(titanic['Embarked'].mode()[0], inplace=True)

# Convert categorical variables to numerical
titanic = pd.get_dummies(titanic, columns=['Sex', 'Embarked'], drop_first=True)

# Define features and target variable
X = titanic.drop('Survived', axis=1)
y = titanic['Survived']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Instantiate the logistic regression model
logistic_model = LogisticRegression(max_iter=1000)

# Train the model
logistic_model.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_pred = logistic_model.predict(X_test_scaled)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Print classification report
print(classification_report(y_test, y_pred))





"""import numpy as np
import pandas as pd
df = pd.read_csv('cars.csv')
df.head()

Logistic regression
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA, TruncatedSVD
import matplotlib.patches as mpatches
import time

# Classifier Libraries
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import collections


# Other Libraries
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import NearMiss
from imblearn.metrics import classification_report_imbalanced
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report
from collections import Counter
from sklearn.model_selection import KFold, StratifiedKFold
import warnings
warnings.filterwarnings("ignore")


df = pd.read_csv('/creditcard.csv')
df.head()

# Good No Null Values!
df.isnull().sum().max()

df.columns

print('No Frauds', round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')
print('Frauds', round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')

colors = ["#0101DF", "#DF0101"]

sns.countplot('Class')
plt.title('Class Distributions \n (0: No Fraud || 1: Fraud)', fontsize=14)

fig, ax = plt.subplots(1, 2, figsize=(18,4))

amount_val = df['Amount'].values
time_val = df['Time'].values

sns.distplot(amount_val, ax=ax[0], color='r')
ax[0].set_title('Distribution of Transaction Amount', fontsize=14)
ax[0].set_xlim([min(amount_val), max(amount_val)])

sns.distplot(time_val, ax=ax[1], color='b')
ax[1].set_title('Distribution of Transaction Time', fontsize=14)
ax[1].set_xlim([min(time_val), max(time_val)])



plt.show()

"""Linear regression

"""

import numpy as np
import pandas as pd
from pandas import Series,DataFrame
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv("/Salary_Data.csv")

df

df.head()



df.info()

df.describe()

df.isnull().sum()

df.skew()

df.kurt()

sns.heatmap(df.corr(),annot=True)
plt.show()

sns.pairplot(df)
plt.show()

plt.plot(df, linestyle = '--', linewidth='5.7')

plt.show()

plt.plot(df, linestyle = '--', linewidth='5.7', color='#FF1493')

plt.show()

df.plot.line(linestyle = ':', linewidth='3')

plt.title('YearExperiene-VS-Salary')
plt.show()

X=df.drop('Salary',axis=1)

y=df.Salary

X.head()

y.head()

from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=0,test_size=0.30)

print(X_train.shape)



X_test.shape

from sklearn.linear_model import LinearRegression
LR=LinearRegression()
LR.fit(X_train,y_train)
LR.fit(X_train,y_train)
LR.coef_
y_pred=LR.predict(X_test)
y_pred
y_test

from sklearn import metrics
R2=metrics.r2_score(y_test,y_pred)
print(metrics.mean_absolute_error(y_test,y_pred))

sns.heatmap(df.corr(),annot=True)
plt.show()



"""support vector machine

"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.metrics import accuracy_score

"""Data Collection and Analysis

"""

diabetes_dataset = pd.read_csv('/diabetes.csv')

diabetes_dataset.head()

# number of rows and Columns in this dataset
diabetes_dataset.shape

diabetes_dataset.describe()

diabetes_dataset['Outcome'].value_counts()

diabetes_dataset.groupby('Outcome').mean()

X = diabetes_dataset.drop(columns = 'Outcome', axis=1)
Y = diabetes_dataset['Outcome']

print(X)

print(Y)

"""Data Standardization

"""

scaler = StandardScaler()

scaler.fit(X)

standardized_data = scaler.transform(X)

print(standardized_data)

X = standardized_data
Y = diabetes_dataset['Outcome']

print(X)
print(Y)

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, stratify=Y, random_state=2)

print(X.shape, X_train.shape, X_test.shape)

classifier = svm.SVC(kernel='linear')

classifier.fit(X_train, Y_train)

# accuracy score on the training data
X_train_prediction = classifier.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)

print('Accuracy score of the training data : ', training_data_accuracy)

X_test_prediction = classifier.predict(X_test)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)

print('Accuracy score of the test data : ', test_data_accuracy)

input_data = (5,166,72,19,175,25.8,0.587,51)

# changing the input_data to numpy array
input_data_as_numpy_array = np.asarray(input_data)

# reshape the array as we are predicting for one instance
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

# standardize the input data
std_data = scaler.transform(input_data_reshaped)
print(std_data)

prediction = classifier.predict(std_data)
print(prediction)

if (prediction[0] == 0):
  print('The person is not diabetic')
else:
  print('The person is diabetic')

classifier.fit(X_train, Y_train)

"""BAGGING AND BOOSTING IN RANDOM forest

"""

import numpy as np
import pandas as pd
from sklearn.datasets import make_classification
from sklearn.ensemble import BaggingClassifier,RandomForestClassifier
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

X,y = make_classification(n_features=5, n_redundant=0, n_informative=5,n_clusters_per_class=1)


df = pd.DataFrame(X,columns=['col1','col2','col3','col4','col5'])
df['target'] = y
print(df.shape)
df.head()

bag = BaggingClassifier(max_features=2)


bag.fit(df.iloc[:,:5],df.iloc[:,-1])

plt.figure(figsize=(12,12))
plot_tree(bag.estimators_[0])
plt.show()

rf = RandomForestClassifier(max_features=2)


rf.fit(df.iloc[:,:5],df.iloc[:,-1])

plt.figure(figsize=(12,12))
plot_tree(rf.estimators_[4])
plt.show()

